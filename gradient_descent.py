"""

y축이 오차이고, x축이 가중치인 2차원 평면을 생각해보자.
우리의 목표는 y값이 최소가되는 x값을 찾아내는 것이다.

당장 아래 코드를 실행시켜보자.

"""

x = 10
lr = 0.0001

for i in range(0, 30000):
    g = (2 * x) - 4 # (x-2)^2의 도함수로부터 기울기 구함
    x += (-g) * lr  # 기울기에 -1과 학습률을 곱한다.(가중치 갱신)
    print(f"f({x}) -> {g}")


"""

y = (x-2)^2의 극솟값은 x=2일 때 0이라는 사실은 자명하다.
위 코드를 실행시켜보면 x가 2에 수렴하고 y값, 즉 오차 또한 0에 수렴하는 것을 확인할 수 있다. 
이것이 바로 경사하강법이다. 기울기에 -1과 학습률을 곱한 후 이를 기존의 가중치에 더해 갱신하는 것이다.

이러한 경사하강법은 크게 3가지로 구분된다.

(1) 배치 경사하강법

전체 데이터세트에 대해 한번만 기울기를 계산하는 방법이다.
에폭 한번에 모든 훈련 데이터 세트를 사용하므로 학습이 오래 걸린다는 단점이 있다.

(2) 확률적 경사하강법

전체 데이터에서 임의로 데이터 일부를 선택하여 기울기를 계산하는 방법이다.
배치 경사하강법에 비해 적은 데이터를 사용하므로 학습 속도가 빠르다. 다만 정확도는 배치 경사하강법보다 낮을 수 있다.

(3) 미니배치 경사하강법

전체 데이터세트를 미니 배치 여러개로 나누고 미니배치 한 개마다 기울기를 구한 후, 평균 기울기를 이용하여 모델을 개선한다.
당연히 전체 데이터 세트를 모두 활용하는 것보다 빠르고, 확률적 경사하강법보다 정확도 측면에서 안정적이다.
따라서 실제로 가장 많이 사용하는 방법이다.

"""