import torch
from torch import nn

"""

시그모이드 함수는 선형 함수의 결과를 [0, 1] 사이에서 비선형 형태로 변형한다.
주로 로지스틱 회귀와 같은 분류 문제에서 결과를 확률로 표현하기 위해 사용한다.
다만, 딥러닝 모델의 깊이가 깊어지면 기울기 소실 문제가 발생하여 딥러닝 모델에서는 선호되지 않는다.

"""

print("### Sigmoid ###")
print(f"[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] -> {torch.sigmoid(torch.tensor([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]))}")

"""

하이퍼볼릭탄젠트 함수는 시그모이드와 비슷하지만 범위가 [-1, 1]로 늘어난다.
시그모이드에서 결과의 평균이 0이 아닌 양수로 편향되는 문제를 해결하였으나 역시 기울기 소실로 인해 선호되지 않는다.

"""

print("### Hyperbolic tangent ###")
print(f"[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] -> {torch.tanh(torch.tensor([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]))}")

"""

렐루 함수는 입력이 음수일 때는 0을 반환하고 양수일 때는 그대로 출력한다.
경사하강법에 영향을 주지 않아 학습 속도가 빠르고 기울기 소멸 문제가 발생하지 않으므로 은닉층에서 애용된다.
다만, 음수 입력의 출력이 항상 0이므로 학습 능력이 감소(Dying ReLU 문제)하는데 이를 해결하기 위해 리키 렐루가 탄생했다.

"""

print("### ReLU ###")
print(f"[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] -> {torch.relu(torch.tensor([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]))}")

"""

리키 렐루는 Dying ReLU 문제를 해결하기 위해 개선된 렐루 함수이다.
0과 양수에 대한 동작은 렐루와 동일하지만 음수일 경우에는 작은 상수(보통 a=0.01로 둔다)를 곱해 0이 되지 않도록 보정한다.

"""

print("### Leaky ReLU ###")
leaky_relu = nn.LeakyReLU()  # LeakyReLU 인스턴스 생성, 리키 렐루는 별도의 함수로는 제공되지 않는다. 
x = torch.tensor([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]).float()
print(f"[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] -> {leaky_relu(x)}")


"""

소프트맥스는 입력 값을 [0, 1] 사이로 정규화하여 출력 값의 총합이 항상 1이 되도록 변환한다.
보통 출력층의 활성화 함수로 많이 채택된다.

"""

softmax = nn.Softmax(dim=0)
print(f"[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] -> {softmax(x)}")
print(f"Sum of all element: {softmax(x).sum()}")    # 부동소수점 표현으로 인해 정확히 1이 나오지는 않는다.